{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using Neural Networks\n",
    "\n",
    "<img style=\"float: left;\" width=\"600\" src=\"images/brain.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "* In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers.\n",
    "* It is a type of linear classifier (a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector)\n",
    "    * $\\mathbf {w} \\cdot \\mathbf {x} +b$\n",
    "\n",
    "<img style=\"float: left;\" width=\"600\" src=\"images/perceptron_node.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation function\n",
    "\n",
    "* The output of a perceptron is a linear function of the input\n",
    "    * However, we want only to trigger the neuron when some condition is fulfilled (that is how our brain works)\n",
    "    * The activation function allows to mimic that behavior, avoiding -inf/+inf as output\n",
    "* In artificial neural networks, the activation function is a equation that defines the output of that node given an input or set of inputs\n",
    "* For the perceptron, activation function is defined as:\n",
    "$f(\\mathbf {x} )={\\begin{cases}1&{\\text{if }}\\ \\mathbf {w} \\cdot \\mathbf {x} +b>0,\\\\0&{\\text{otherwise}}\\end{cases}}$\n",
    "\n",
    "* where $\\mathbf {w}$  is a vector of real-valued weight and $b$ is the bias. \n",
    "\n",
    "    * The bias shifts the decision boundary away from the origin\n",
    "\n",
    "* Alternative activation functions:\n",
    "    * Sigmoid or Logistic\n",
    "    * Tanh — Hyperbolic tangent\n",
    "    * ReLu -Rectified linear units\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # package for scientific computing in Python (similar to Matlab)\n",
    "\n",
    "def perceptron(x, w, b):\n",
    "    return ((w * x) + b)\n",
    "\n",
    "def activation(perceptron_values):\n",
    "    y = perceptron_values >= 0.0 #returns 1 if value > 0, otherwise returns 0\n",
    "    return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "* For perceptrons, hinge loss is used as the loss function:\n",
    "\n",
    "\\begin{equation}\\mathcal{L}(y) = \\max(0, 1 - y \\cdot \\hat{y}) \\end{equation}\n",
    "\n",
    "* Lets write the perceptron output again:\n",
    "\n",
    "\\begin{equation} f(\\mathbf {x} ) = \\hat{y} = {\\begin{cases}1&{\\text{if }}\\ \\mathbf {w} \\cdot \\mathbf {x} +b>0,\\\\0&{\\text{otherwise}}\\end{cases}} \\end{equation}\n",
    "\n",
    "* The derivatives with respect of  $w$ and $b$ are:\n",
    "\n",
    "\\begin{equation} \\frac{\\partial \\mathcal{L} }{\\partial w}=\\begin{cases}-y\\cdot x&{\\text{if }} y \\cdot \\hat{y}<1\\\\0&{\\text{otherwise}}\\end{cases} \\end{equation}\n",
    "\n",
    "TEM QUE RESOLVER ESSA COISA!\n",
    "LOSS NAO CHEGA NA REGRA DE ATUALIZACAO\n",
    "\n",
    "### Training\n",
    "\n",
    "* Training a perceptron means updating the weight and bias vectors in order to minimize the loss function\n",
    "* The minimization can be achieved by using gradient descent\n",
    "* Gradient Descent algorithm:\n",
    "\n",
    "\\begin{equation} \\theta' = \\theta - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_i} \\begin{equation}\n",
    "\n",
    "* We have a linear function $\\mathbf{w} \\cdot \\mathbf {x} +b$, therefore the update rule with respect to $w$ is:\n",
    "\n",
    "\\begin{equation} \\mathbf{w}' = \\mathbf{w} - \\alpha \\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}} \\end{equation}\n",
    "\n",
    "\\begin{equation} \\mathbf{w}' = \\mathbf{w} - \\alpha (y - \\hat{y}) * x\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(x, y, w, learning_rate):\n",
    "    w = w + learning_rate * (expected - predicted) * x\n",
    "    return w\n",
    "\n",
    "def update_weights(x, y, b, learning_rate):\n",
    "    bias = bias + learning_rate * (expected - predicted)\n",
    "    return b\n",
    "\n",
    "def train(x, y, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
